用fastapi搭建的服务怎么考虑并发情况？


用 FastAPI 搭服务时，“并发”主要要从 4 层一起考虑：**应用代码是不是阻塞**、**ASGI 服务器/worker 怎么配**、**下游资源（DB/Redis/外部 API）怎么限流和复用连接**、**整体的背压与隔离**。下面给你一套实战视角的清单。

---

## 1) 先搞清楚：FastAPI 的并发模型是什么

* FastAPI 跑在 **ASGI** 上（常见是 Uvicorn / Hypercorn），单个进程里是 **事件循环 + 协程**。
* 你写的 `async def` 只有在**遇到 await 且 await 的东西是非阻塞**时，才能把执行权交出去，让同一个进程里同时“跑”很多请求。
* 如果你在 `async def` 里做了阻塞操作（比如 `time.sleep()`、同步 requests、同步 DB 查询、重 CPU 计算），事件循环就被卡住了：**看似 async，实际把并发打没了**。

---

## 2) 代码层：避免阻塞是并发的生命线

### A. I/O（HTTP/DB/Redis）统一用异步库

* 外部 HTTP：用 `httpx.AsyncClient`
* DB：用 async driver（例如 SQLAlchemy async + asyncpg / aiomysql 等）
* Redis：redis-py 的 asyncio 接口（`redis.asyncio`）

**关键点**：不要在 `async def` 里用同步库（requests、pymysql、psycopg2 等），否则会把整个事件循环拖死。

### B. CPU 密集型任务不要塞进请求线程/事件循环

比如：大 JSON 处理、图像处理、压缩、机器学习推理（没用异步/没释放 GIL 的那种）
做法：

* **进程池**：`ProcessPoolExecutor`
* 或者把重活丢给 **Celery/RQ/Arq** 做异步任务队列
* FastAPI 内部也可以 `run_in_threadpool`（适合短的同步 I/O 或轻 CPU），但别当成万能药

### C. 正确使用 BackgroundTasks（但它不是任务队列）

`BackgroundTasks` 只是把函数放到同一进程里晚点执行：

* 适合：发通知、写日志、轻量收尾
* 不适合：耗时/高并发/需要可靠重试的任务（那就上队列）

---

## 3) 服务器层：worker、线程、连接数怎么配

### A. worker 数量：靠多进程吃满多核

单个 worker 是一个进程 + 一个事件循环（并发 I/O 很强，但 CPU 卡住就全卡）。

* 经验值：`workers = CPU核数` 或 `CPU核数 * 2`（看任务 CPU 比例）
* 纯 I/O 服务：可以更偏多 worker，但别忘了下游（DB）会被打爆

### B. 不要误用 “多线程提升 async 并发”

对 async 服务来说，**并发主要来自事件循环**，不是线程数。

* 多线程更适合：你大量同步阻塞（但这本质是补锅，不是最佳方案）

### C. 连接池与限制

并发上去后，瓶颈常常在下游：

* DB 连接池（async engine pool_size / max_overflow）
* HTTP client 的连接池与并发限制（httpx limits）
* Redis 连接池
  做“容量规划”时要算：
  **请求并发峰值 × 每个请求平均占用下游连接数** <= 下游连接池能力

---

## 4) 下游资源：限流、超时、重试、熔断（背压体系）

### A. 必须设置超时（否则并发越高死得越快）

* HTTP 调用：connect/read/write timeout
* DB：statement timeout / connect timeout
* 你的接口：整体 request timeout（由网关/反代控制也行）

### B. 限流与并发闸门（推荐）

典型做法是在“慢依赖”前面加闸门，防止把自己和下游打爆：

* 本地：`asyncio.Semaphore`（限制某类操作并发数）
* 分布式：Redis 限流（令牌桶/漏桶）或网关限流

### C. 熔断/降级

如果外部 API/DB 抖了：

* 快速失败（返回友好错误/降级数据）
* 避免大量请求堆积导致线程/连接耗尽

---

## 5) 数据一致性与共享状态：并发下最容易踩坑

* **不要用全局变量存请求态数据**（会被并发请求互相污染）
* 缓存/计数器要用原子操作或外部存储（Redis、DB）
* 如果你在内存里做 LRU cache：注意多 worker 是**多份内存**，彼此不共享

---

## 6) 观测与压测：别靠感觉

并发问题最终要靠数据说话：

* 指标：QPS、p95/p99、错误率、事件循环延迟、DB pool 使用率、外部依赖耗时
* 压测：k6 / Locust / wrk（看你的协议与场景）
* 关键观测点：**是否出现排队**（连接池耗尽、Semaphore 排队、网关队列）

---

## 7) 一个很实用的“并发排查”顺序

1. 先看是否在 `async def` 里做了同步阻塞（最常见）
2. 看下游：DB/HTTP 的连接池是否耗尽（等待时间拉长）
3. 看 worker 数：CPU 是否满、单 worker 是否被阻塞
4. 看超时与重试：是否导致请求“堆积雪球”
5. 加闸门（Semaphore/限流）+ 调小重试 + 加熔断

---

如果你愿意给我两点信息，我可以把建议落到“怎么配参数/怎么改代码”的级别（不需要你贴全部代码）：

1. 你的服务主要是 **I/O 型**（DB/外部 API）还是 **CPU 型**（计算/推理）？
2. 你现在用的是 `uvicorn` 单起还是 `gunicorn -k uvicorn.workers.UvicornWorker` 多 worker？
