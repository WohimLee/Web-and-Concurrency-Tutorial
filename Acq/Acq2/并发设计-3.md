我这边是跨境B2B平台，的服务应该还是API 调用比较多，CPU密集型很少，手上有两个服务，分别要放到不同的镜像，根据我的场景给我设计并发方案： 
1. 服务一 
1.1.shadowHunter-租户对客户的背调核验请求（LLM、search API） 
1.2.nexusAssess-租户对客户的风险报告生成请求（LLM API） 
2. 服务二 
2.1.omniRAG-自动化处理文档（前端上传发过来的请求，带PDF OSS链接）、入库（LLM、embedding API） 
2.2.dealBrain-商品搜索（询盘、推荐都会调）、推荐(特定到产品推荐阶段调用)（LLM 、reranker API， 数据库查询）


明白，你这个更像“**I/O 密集型 API 编排服务**”：瓶颈主要在 **LLM / search / embedding / reranker / DB** 这些下游，而不是 FastAPI 自身 CPU。所以并发方案的核心是：

* **异步化 I/O**（httpx async、async DB driver）
* **连接池 + 超时**（避免请求堆积）
* **分依赖限并发（Semaphore）+ 背压（429/队列上限）**
* **把“长任务/批处理”从在线请求里剥离**（尤其 omniRAG）

下面我按你两套镜像（两个服务）分别设计，并给出“默认可用”的参数起点。

---

# 总体原则（两套服务都适用）

## 1) 你的并发上限 ≈ min(下游限额, 连接池, 你愿意承受的排队)

**不要用“无限 await”去并发打下游**，那会把自己和下游一起打挂。

建议你对每类下游都做一个“并发预算”：

* LLM API：`k_llm`
* search API：`k_search`
* embedding API：`k_embed`
* reranker API：`k_rank`
* DB：`pool_size` 与 `max_overflow`

并发预算的典型起点（后面会按服务细分）：

* LLM：每个 worker `2~8`（看 QPS/成本/限额）
* search：每个 worker `5~20`
* embedding：每个 worker `5~30`（通常限额更宽、但吞吐要看 payload）
* reranker：每个 worker `5~15`
* DB：连接池 `pool_size=10~30`（看 SQL 重不重 & DB 侧连接限制）

## 2) 必做三件事：超时、重试策略、熔断/降级

* **超时**：连接超时 + 读超时 + 总超时（否则下游慢会把你拖死）
* **重试**：只对 *幂等* 请求重试；对 429/503 做指数退避；对 LLM 生成类通常“轻重试”或不重试
* **熔断/降级**：下游失败率高时快速失败，返回可解释错误或降级结果

## 3) gunicorn 并发不是线程数，而是：workers ×（每 worker async 并发）

I/O 型服务通常：

* `workers = CPU核数` 或 `核数*2`（模型不驻留内存时更敢开）
* 每个 worker 再通过 async + semaphore 控制“对下游的真实并发”

---

# 服务一镜像：shadowHunter / nexusAssess（面向租户的交互式请求）

**定位**：在线交互、对时延敏感、LLM 调用贵 & 速率受限
**目标**：稳定、低尾延迟、严格背压（宁可 429，也不要排队到超时）

## A. 并发策略

### 1) shadowHunter（LLM + search）

典型链路：`search -> 过滤/汇总 -> LLM核验/总结`

* 对 search 并发：可以相对高一些（但要控制“扇出”）
* 对 LLM 并发：严格控制

**建议起点（每个 worker）**

* `k_search = 10`
* `k_llm = 4`
* **单请求内 search 扇出**（比如同时查多个源）：再加一个 `k_fanout = 5`，防止一个请求把 search 并发吃光

### 2) nexusAssess（LLM 报告生成）

典型是单次长调用或多轮调用：

* **并发更要保守**，否则成本和排队都会爆

**建议起点（每个 worker）**

* `k_llm = 2~3`
* 设定**队列上限**：例如“最多排队 50 个请求”，超过返回 429（可带 `Retry-After`）

## B. worker 配置（服务一）

假设容器是 I/O 密集、CPU不重：

* `workers = min(2*CPU核数, 8)` 作为起点（先别无限加）
* 重要的是：你开再多 worker，也可能被 LLM 限额卡住，所以别用 worker 硬顶并发

**启动命令示例**

```bash
gunicorn app.main:app \
  -k uvicorn.workers.UvicornWorker \
  -w 4 \
  -b 0.0.0.0:8000 \
  --timeout 120 \
  --graceful-timeout 30 \
  --keep-alive 5 \
  --max-requests 2000 \
  --max-requests-jitter 200
```

## C. FastAPI 关键实现（服务一）

1. **全局复用 httpx AsyncClient（连接池）**
2. **按下游做 semaphore**
3. **超时 + 429/503 重试（有节制）**

示意代码（简化版）：

```python
import asyncio
import httpx
from fastapi import FastAPI, HTTPException

app = FastAPI()

# 每个worker内的并发闸门
sem_llm = asyncio.Semaphore(4)
sem_search = asyncio.Semaphore(10)

@app.on_event("startup")
async def startup():
    app.state.http = httpx.AsyncClient(
        timeout=httpx.Timeout(connect=5.0, read=60.0, write=10.0, pool=60.0),
        limits=httpx.Limits(max_connections=200, max_keepalive_connections=50),
    )

@app.on_event("shutdown")
async def shutdown():
    await app.state.http.aclose()

async def call_search(params: dict):
    async with sem_search:
        r = await app.state.http.get("https://search.example.com", params=params)
        r.raise_for_status()
        return r.json()

async def call_llm(payload: dict):
    # 如果想做队列上限：这里可以 try_acquire（见下方）
    async with sem_llm:
        r = await app.state.http.post("https://llm.example.com", json=payload)
        if r.status_code == 429:
            raise HTTPException(status_code=429, detail="LLM rate limited")
        r.raise_for_status()
        return r.json()
```

> “队列上限/快速失败”做法：不要无脑 `await sem.acquire()`，而是加超时/try-acquire，超过就 429。

---

# 服务二镜像：omniRAG / dealBrain（一个是批处理，一个是在线检索推荐）

这套要重点做**隔离**：

* omniRAG（文档处理入库）是**吞吐型、可延迟**
* dealBrain（搜索推荐）是**交互型、低延迟**

你虽然放在同一个镜像/服务里，但它们最好“逻辑上隔离资源”，否则批处理会把在线流量拖慢。

---

## 2.1 omniRAG（文档处理、embedding、入库）

**定位**：长任务/批处理/高吞吐，最容易把下游打爆
**最推荐的架构**：把它做成“**异步任务**”，HTTP 只负责提交任务与查询状态。

### A. 并发策略

* 文档拆分、embedding、入库通常是 pipeline
* 你需要的是：

  * embedding 并发闸门：`k_embed`
  * DB 写入并发闸门：`k_db_write`
  * **任务队列长度上限**（防止无限堆积）

**建议起点（每个 worker）**

* `k_embed = 20`
* `k_db_write = 10`
* 每个任务内部并行：比如每次最多并行处理 `batch=32` 个 chunk（别把 chunk 全并发出去）

### B. 关键建议：不要用 FastAPI BackgroundTasks 扛批处理

用真正的队列/worker（哪怕同容器内也行）：

* 简化版：容器内起一个独立“worker 进程”消费 Redis 队列
* 更稳健：Celery/RQ/Arq（Arq 对 async 友好）

HTTP 接口：

* `POST /rag/ingest` -> 返回 task_id
* `GET /tasks/{task_id}` -> 状态、进度、失败原因

### C. 如果你“必须单容器内解决”

至少做：

* 内存队列 `asyncio.Queue(maxsize=N)` 做背压
* worker 协程固定数量（比如 4~16 个），并且内部再用 `k_embed`/`k_db_write` 控制下游

---

## 2.2 dealBrain（商品搜索、推荐：LLM + reranker + DB）

**定位**：在线请求，p95/p99 很重要
典型链路：`DB检索候选 -> rerank -> LLM总结/解释（可选）`

### A. 并发策略

* DB 查询：受连接池影响（不要让请求堆在 pool 等待）
* reranker：通常是外部 API（或内部服务），并发可控
* LLM：如果只是生成“解释/文案”，可以作为“可降级项”

**建议起点（每个 worker）**

* DB pool：`pool_size=20, max_overflow=10`（按你的 DB 限制调整）
* `k_rank = 10`
* `k_llm = 2~4`（如果 LLM 只是“锦上添花”，建议更小并且失败可降级）

### B. 降级策略（强烈建议）

* reranker 超时/失败：退化为“DB score 排序”
* LLM 超时/失败：返回结构化结果 + 空的 explain 字段
  这样在线体验不会被下游抖动拖死。

---

# 两个服务的资源与流量隔离建议（很关键）

## 1) 端点级别隔离（同一服务内）

给 omniRAG 和 dealBrain 配不同 semaphore：

* omniRAG 的 embedding 并发再高，也不要吃光 HTTP client 连接池
* dealBrain 单独保留一部分“关键并发预算”

## 2) 进程级隔离（更推荐）

即使同一个镜像，你也可以：

* 让 gunicorn 起两个实例分别监听不同端口（或不同入口命令）
* 或者直接拆成两个镜像（如果后续允许），让扩缩容更灵活

你现在已经是两个镜像了，这很好：
服务二内部依然要把 omniRAG 和 dealBrain “资源隔离”。

---

# 给你一套“可直接落地”的默认配置（起步版）

## 服务一（shadowHunter / nexusAssess）

* gunicorn：`workers=4`（再按 CPU 调整）
* LLM 并发：每 worker `2~4`
* search 并发：每 worker `10`
* 必须：try-acquire + 队列上限，超过直接 429
* 超时：LLM read 60~120s；search read 10~20s

## 服务二（omniRAG / dealBrain）

* gunicorn：`workers=4~8`（看机器与 DB 限制）
* dealBrain：

  * DB pool：`20/10` 起步
  * reranker 并发：`10`
  * LLM 并发：`2~4`，失败可降级
* omniRAG：

  * **队列化**（强烈建议）
  * embedding 并发：`20`
  * 写库并发：`10`
  * ingestion 入口必须有 max queue size（比如 1000 任务或按租户配额）

---

# 你接下来最该做的两件事（不需要大改架构）

1. **把“下游并发闸门 + 超时 + 降级”加上**（这是稳定性的 80%）
2. **把 omniRAG 从同步请求改为任务式**（否则并发一高就会请求堆积、连接池耗尽、容器 OOM/超时雪崩）

---

如果你愿意，我可以基于你的“实际约束”把上面的数字算得更贴近：

* 你用的 LLM/search/embedding/reranker 各自的 **QPS/并发限额**（或 429 频率）
* DB 是什么（Postgres/MySQL/ES/向量库），允许最大连接数
* 目标：比如 dealBrain 要求 p95 < 800ms？shadowHunter p95 < 5s？

你不提供也没关系：按我上面的“起步版”先落地，基本就能把并发稳定性从“靠运气”变成“可控”。
